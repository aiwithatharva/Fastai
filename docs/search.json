[
  {
    "objectID": "Is_it_a_tennis_Creating_a_model_from_your_own_data.html#is-it-a-tennis",
    "href": "Is_it_a_tennis_Creating_a_model_from_your_own_data.html#is-it-a-tennis",
    "title": "",
    "section": "Is it a tennis?",
    "text": "Is it a tennis?\n\n#NB: Kaggle requires phone verification to use the internet or a GPU. If you haven't done that yet, the cell below will fail\n#    This code is only here to check that your internet is enabled. It doesn't do anything else.\n#    Here's a help thread on getting your phone number verified: https://www.kaggle.com/product-feedback/135367\n\nimport socket,warnings\ntry:\n    socket.setdefaulttimeout(1)   # Sets global timeout time of 1 sec, ie if socket operation is not complteted in 1 sec then error will be raised.\n    # if set to around 0.01 in colab server it give timeout error since the time is too less for socket operation.\n    socket.socket(socket.AF_INET, socket.SOCK_STREAM).connect(('1.1.1.1', 53)) # This will use AF_INET which is same as IPV4, SOCK_STREAM means TCP.\nexcept socket.error as ex:\n    # print(\"Original exception :\", ex)\n    raise Exception(\"STOP: No internet. Click '&gt;|' in top right and set 'Internet' switch to on\")\n# We cached the error here but did not printed it where as we raise a custom Exception.\n\n::: {#cell-3 .cell _kg_hide-input=‘true’ _kg_hide-output=‘true’ execution=‘{“iopub.status.busy”:“2022-08-15T19:54:58.581817Z”,“iopub.execute_input”:“2022-08-15T19:54:58.582154Z”,“iopub.status.idle”:“2022-08-15T19:55:20.002673Z”,“shell.execute_reply.started”:“2022-08-15T19:54:58.58212Z”,“shell.execute_reply”:“2022-08-15T19:55:20.001747Z”}’ trusted=‘true’ outputId=‘0b4b8a64-b932-4861-e183-ca77133bd6c0’ execution_count=2}\n# It's a good idea to ensure you're running the latest version of any libraries you need.\n# `!pip install -Uqq &lt;libraries&gt;` upgrades to the latest version of &lt;libraries&gt;\n# NB: You can safely ignore any warnings or errors pip spits out about running as root or incompatibilities\nimport os\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '') #if kernel run type is not found it will return second arguemnt\n\nif iskaggle:\n    print(\"It is Kaggle !!\")\n\n!pip install -Uqq fastai # -Uqq refers to upgrade extra quitely\n!pip install duckduckgo_search==6.2.6\n\nCollecting duckduckgo_search==6.2.6\n  Downloading duckduckgo_search-6.2.6-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: click&gt;=8.1.7 in /usr/local/lib/python3.10/dist-packages (from duckduckgo_search==6.2.6) (8.1.7)\nCollecting primp&gt;=0.5.5 (from duckduckgo_search==6.2.6)\n  Downloading primp-0.5.5-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nDownloading duckduckgo_search-6.2.6-py3-none-any.whl (27 kB)\nDownloading primp-0.5.5-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/3.0 MB ? eta -:--:--   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸ 3.0/3.0 MB 103.5 MB/s eta 0:00:01   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/3.0 MB 60.6 MB/s eta 0:00:00\nInstalling collected packages: primp, duckduckgo_search\nSuccessfully installed duckduckgo_search-6.2.6 primp-0.5.5\n\n:::\nIn 2015 the idea of creating a computer system that could recognise tenniss was considered so outrageously challenging that it was the basis of this XKCD joke:\nBut today, we can do exactly that, in just a few minutes, using entirely free resources!\nThe basic steps we’ll take are:\n\nUse DuckDuckGo to search for images of “tennis photos”\nUse DuckDuckGo to search for images of “badminton photos”\nFine-tune a pretrained neural network to recognise these two groups\nTry running this model on a picture of a tennis and see if it works."
  },
  {
    "objectID": "Is_it_a_tennis_Creating_a_model_from_your_own_data.html#step-1-download-images-of-tenniss-and-non-tenniss",
    "href": "Is_it_a_tennis_Creating_a_model_from_your_own_data.html#step-1-download-images-of-tenniss-and-non-tenniss",
    "title": "",
    "section": "Step 1: Download images of tenniss and non-tenniss",
    "text": "Step 1: Download images of tenniss and non-tenniss\n::: {#cell-7 .cell _kg_hide-input=‘true’ execution=‘{“iopub.status.busy”:“2022-08-15T20:17:04.164811Z”,“iopub.execute_input”:“2022-08-15T20:17:04.165128Z”,“iopub.status.idle”:“2022-08-15T20:17:04.171Z”,“shell.execute_reply.started”:“2022-08-15T20:17:04.165074Z”,“shell.execute_reply”:“2022-08-15T20:17:04.170146Z”}’ trusted=‘true’ execution_count=3}\nfrom duckduckgo_search import DDGS # previous function is depreciated this is the new one.\nfrom fastcore.all import *\n\ndef search_images(term, max_images=30):\n    print(f\"Searching for '{term}'\")\n    return L(DDGS().images(term, max_results=max_images)).itemgot('image')\n    #L before just right to L is a list type in fastai, DDGS is object has a functino images with input term \"term\" and max_results argument giving number of images to give.\n    # from each item in item itemgot('image') will retrieve image key whose value is url\n:::\nLet’s start by searching for a tennis photo and seeing what kind of result we get. We’ll start by getting URLs from a search:\n\n#NB: `search_images` depends on duckduckgo.com, which doesn't always return correct responses.\n#    If you get a JSON error, just try running it again (it may take a couple of tries).\nurls = search_images('tennis photos', max_images=1)\nurls[0]\n\nSearching for 'tennis photos'\n\n\n'https://images.wallpapersden.com/image/download/rafael-nadal-tennis-tennis-player_Z2ZrbmmUmZqaraWkpJRoZWVlrWdlZWU.jpg'\n\n\n…and then download a URL and take a look at it:\n\nfrom fastdownload import download_url\ndest = 'tennis.jpg'\ndownload_url(urls[0], dest, show_progress=False)\n\nfrom fastai.vision.all import *\nim = Image.open(dest)\nim.to_thumb(256,256)\n\n\n\n\n\n\n\n\nNow let’s do the same with “badminton photos”:\n\ndownload_url(search_images('badminton photos', max_images=1)[0], 'badminton.jpg', show_progress=False) # search image returns list of dict, hence we are accessing url in image key\nImage.open('badminton.jpg').to_thumb(256,256)\n\nSearching for 'badminton photos'\n\n\n\n\n\n\n\n\n\nOur searches seem to be giving reasonable results, so let’s grab a few examples of each of “tennis” and “badminton” photos, and save each group of photos to a different folder (I’m also trying to grab a range of lighting conditions here):\n\nsearches = 'badminton','tennis' # this is a tuple not list\npath = Path('tennis_or_not') # thus function gives path which will be used further\nprint(type(path))\n\n&lt;class 'pathlib.PosixPath'&gt;\n\n\n\nfrom time import sleep\n\nfor o in searches:\n    dest = (path/o) # this will append the path with the catagory but the dest will still be pathlib object and not a string\n    dest.mkdir(exist_ok=True, parents=True) # since dest is pathlib obj we can use mkdir directly, by exist_ok=True it does not raise error when the directory already exists.\n    # parents=True will create any parent if needed\n    download_images(dest, urls=search_images(f'{o} photo'))\n    sleep(10)  # Pause between searches to avoid over-loading server\n    download_images(dest, urls=search_images(f'{o} sun photo'))\n    sleep(10)\n    download_images(dest, urls=search_images(f'{o} shade photo'))\n    sleep(10)\n    resize_images(path/o, max_size=400, dest=path/o)\n\nSearching for 'badminton photo'\nSearching for 'badminton sun photo'\nSearching for 'badminton shade photo'\nSearching for 'tennis photo'\nSearching for 'tennis sun photo'\nSearching for 'tennis shade photo'"
  },
  {
    "objectID": "Is_it_a_tennis_Creating_a_model_from_your_own_data.html#step-2-train-our-model",
    "href": "Is_it_a_tennis_Creating_a_model_from_your_own_data.html#step-2-train-our-model",
    "title": "",
    "section": "Step 2: Train our model",
    "text": "Step 2: Train our model\nSome photos might not download correctly which could cause our model training to fail, so we’ll remove them:\n\nfailed = verify_images(get_image_files(path)) # verify function will check if for ach image is it a valid image format of not\nfailed.map(Path.unlink) # It image format is not value then then it is collected in failed list.\n                        # .map will apply the Path.unlink function to all the failed images and delete the failed images\nlen(failed)\n\n9\n\n\nTo train a model, we’ll need DataLoaders, which is an object that contains a training set (the images used to create a model) and a validation set (the images used to check the accuracy of a model – not used during training). In fastai we can create that easily using a DataBlock, and view sample images from it:\n\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=[Resize(192, method='squish')]\n).dataloaders(path, bs=32)\n\ndls.show_batch(max_n=6)\n\n\n\n\n\n\n\n\nHere what each of the DataBlock parameters means:\nblocks=(ImageBlock, CategoryBlock),\nThe inputs to our model are images, and the outputs are categories (in this case, “tennis” or “badminton”).\nget_items=get_image_files,\nTo find all the inputs to our model, run the get_image_files function (which returns a list of all image files in a path).\nsplitter=RandomSplitter(valid_pct=0.2, seed=42),\nSplit the data into training and validation sets randomly, using 20% of the data for the validation set.\nget_y=parent_label,\nThe labels (y values) is the name of the parent of each file (i.e. the name of the folder they’re in, which will be tennis or badminton).\nitem_tfms=[Resize(192, method='squish')]\nBefore training, resize each image to 192x192 pixels by “squishing” it (as opposed to cropping it).\nNow we’re ready to train our model. The fastest widely used computer vision model is resnet18. You can train this in a few minutes, even on a CPU! (On a GPU, it generally takes under 10 seconds…)\nfastai comes with a helpful fine_tune() method which automatically uses best practices for fine tuning a pre-trained model, so we’ll use that.\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(10)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.435960\n1.422994\n0.483871\n00:00\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.686919\n1.147038\n0.419355\n00:00\n\n\n1\n0.546053\n0.888334\n0.322581\n00:00\n\n\n2\n0.383824\n0.769488\n0.290323\n00:01\n\n\n3\n0.303360\n0.660374\n0.193548\n00:01\n\n\n4\n0.243147\n0.658344\n0.193548\n00:01\n\n\n5\n0.199071\n0.660686\n0.161290\n00:01\n\n\n6\n0.163785\n0.667949\n0.129032\n00:00\n\n\n7\n0.138104\n0.669258\n0.129032\n00:01\n\n\n8\n0.119873\n0.661155\n0.129032\n00:00\n\n\n9\n0.103388\n0.651775\n0.129032\n00:00\n\n\n\n\n\nGenerally when I run this I see 100% accuracy on the validation set (although it might vary a bit from run to run).\n“Fine-tuning” a model means that we’re starting with a model someone else has trained using some other dataset (called the pretrained model), and adjusting the weights a little bit so that the model learns to recognise your particular dataset. In this case, the pretrained model was trained to recognise photos in imagenet, and widely-used computer vision dataset with images covering 1000 categories) For details on fine-tuning and why it’s important, check out the free fast.ai course."
  },
  {
    "objectID": "Is_it_a_tennis_Creating_a_model_from_your_own_data.html#step-3-use-our-model-and-build-your-own",
    "href": "Is_it_a_tennis_Creating_a_model_from_your_own_data.html#step-3-use-our-model-and-build-your-own",
    "title": "",
    "section": "Step 3: Use our model (and build your own!)",
    "text": "Step 3: Use our model (and build your own!)\nLet’s see what our model thinks about that tennis we downloaded at the start:\n\nis_tennis,_,probs = learn.predict(PILImage.create('tennis.jpg'))  # PILImage will return Tensor Image using pillow library\nprint(f\"This is a: {is_tennis}.\")\nprint(f\"Probability it's a tennis: {probs[0]:.4f}\")\n\n\n\n\n\n\n\n\nThis is a: tennis.\nProbability it's a tennis: 0.0002\n\n\nGood job, resnet18. :)\nSo, as you see, in the space of a few years, creating computer vision classification models has gone from “so hard it’s a joke” to “trivially easy and free”!\nIt’s not just in computer vision. Thanks to deep learning, computers can now do many things which seemed impossible just a few years ago, including creating amazing artworks, and explaining jokes. It’s moving so fast that even experts in the field have trouble predicting how it’s going to impact society in the coming years.\nOne thing is clear – it’s important that we all do our best to understand this technology, because otherwise we’ll get left behind!\nNow it’s your turn. Click “Copy & Edit” and try creating your own image classifier using your own image searches!\nIf you enjoyed this, please consider clicking the “upvote” button in the top-right – it’s very encouraging to us notebook authors to know when people appreciate our work."
  },
  {
    "objectID": "2020-02-20-test.html",
    "href": "2020-02-20-test.html",
    "title": "Fastpages Notebook Blog Post",
    "section": "",
    "text": "A tutorial of fastpages for Jupyter notebooks."
  },
  {
    "objectID": "2020-02-20-test.html#front-matter",
    "href": "2020-02-20-test.html#front-matter",
    "title": "Fastpages Notebook Blog Post",
    "section": "Front Matter",
    "text": "Front Matter\nThe first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this:\n# \"My Title\"\n&gt; \"Awesome summary\"\n\n- toc: true\n- branch: master\n- badges: true\n- comments: true\n- author: Hamel Husain & Jeremy Howard\n- categories: [fastpages, jupyter]\n\nSetting toc: true will automatically generate a table of contents\nSetting badges: true will automatically include GitHub and Google Colab links to your notebook.\nSetting comments: true will enable commenting on your blog post, powered by utterances.\n\nThe title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README."
  },
  {
    "objectID": "2020-02-20-test.html#markdown-shortcuts",
    "href": "2020-02-20-test.html#markdown-shortcuts",
    "title": "Fastpages Notebook Blog Post",
    "section": "Markdown Shortcuts",
    "text": "Markdown Shortcuts\nA #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post.\nA #hide_input comment at the top of any code cell will only hide the input of that cell.\n\n#hide_input\nprint('The comment #hide_input was used to hide the code that produced this.')\n\nThe comment #hide_input was used to hide the code that produced this.\n\n\nput a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it:\n\n#collapse-hide\nimport pandas as pd\nimport altair as alt\n\nput a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it:\n\n#collapse-show\ncars = 'https://vega.github.io/vega-datasets/data/cars.json'\nmovies = 'https://vega.github.io/vega-datasets/data/movies.json'\nsp500 = 'https://vega.github.io/vega-datasets/data/sp500.csv'\nstocks = 'https://vega.github.io/vega-datasets/data/stocks.csv'\nflights = 'https://vega.github.io/vega-datasets/data/flights-5k.json'\n\nplace a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it:\n\n#collapse-output\nprint('The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.')\n\nThe comment #collapse-output was used to collapse the output of this cell by default but you can expand it."
  },
  {
    "objectID": "2020-02-20-test.html#interactive-charts-with-altair",
    "href": "2020-02-20-test.html#interactive-charts-with-altair",
    "title": "Fastpages Notebook Blog Post",
    "section": "Interactive Charts With Altair",
    "text": "Interactive Charts With Altair\nCharts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook.\n\n# hide\ndf = pd.read_json(movies) # load movies data\ndf.columns = [x.replace(' ', '_') for x in df.columns.values]\ngenres = df['Major_Genre'].unique() # get unique field values\ngenres = list(filter(lambda d: d is not None, genres)) # filter out None values\ngenres.sort() # sort alphabetically\n\n\n#hide\nmpaa = ['G', 'PG', 'PG-13', 'R', 'NC-17', 'Not Rated']\n\n\nExample 1: DropDown\n\n# single-value selection over [Major_Genre, MPAA_Rating] pairs\n# use specific hard-wired values as the initial selected values\nselection = alt.selection_single(\n    name='Select',\n    fields=['Major_Genre', 'MPAA_Rating'],\n    init={'Major_Genre': 'Drama', 'MPAA_Rating': 'R'},\n    bind={'Major_Genre': alt.binding_select(options=genres), 'MPAA_Rating': alt.binding_radio(options=mpaa)}\n)\n  \n# scatter plot, modify opacity based on selection\nalt.Chart(df).mark_circle().add_selection(\n    selection\n).encode(\n    x='Rotten_Tomatoes_Rating:Q',\n    y='IMDB_Rating:Q',\n    tooltip='Title:N',\n    opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05))\n)\n\n\n\n\n\n\n\n\nExample 2: Tooltips\n\nalt.Chart(df).mark_circle().add_selection(\n    alt.selection_interval(bind='scales', encodings=['x'])\n).encode(\n    alt.X('Rotten_Tomatoes_Rating', type='quantitative'),\n    alt.Y('IMDB_Rating', type='quantitative', axis=alt.Axis(minExtent=30)),\n#     y=alt.Y('IMDB_Rating:Q', ), # use min extent to stabilize axis title placement\n    tooltip=['Title:N', 'Release_Date:N', 'IMDB_Rating:Q', 'Rotten_Tomatoes_Rating:Q']\n).properties(\n    width=500,\n    height=400\n)\n\n\n\n\n\n\n\n\nExample 3: More Tooltips\n\n# select a point for which to provide details-on-demand\nlabel = alt.selection_single(\n    encodings=['x'], # limit selection to x-axis value\n    on='mouseover',  # select on mouseover events\n    nearest=True,    # select data point nearest the cursor\n    empty='none'     # empty selection includes no data points\n)\n\n# define our base line chart of stock prices\nbase = alt.Chart().mark_line().encode(\n    alt.X('date:T'),\n    alt.Y('price:Q', scale=alt.Scale(type='log')),\n    alt.Color('symbol:N')\n)\n\nalt.layer(\n    base, # base line chart\n    \n    # add a rule mark to serve as a guide line\n    alt.Chart().mark_rule(color='#aaa').encode(\n        x='date:T'\n    ).transform_filter(label),\n    \n    # add circle marks for selected time points, hide unselected points\n    base.mark_circle().encode(\n        opacity=alt.condition(label, alt.value(1), alt.value(0))\n    ).add_selection(label),\n\n    # add white stroked text to provide a legible background for labels\n    base.mark_text(align='left', dx=5, dy=-5, stroke='white', strokeWidth=2).encode(\n        text='price:Q'\n    ).transform_filter(label),\n\n    # add text labels for stock prices\n    base.mark_text(align='left', dx=5, dy=-5).encode(\n        text='price:Q'\n    ).transform_filter(label),\n    \n    data=stocks\n).properties(\n    width=500,\n    height=400\n)"
  },
  {
    "objectID": "2020-02-20-test.html#data-tables",
    "href": "2020-02-20-test.html#data-tables",
    "title": "Fastpages Notebook Blog Post",
    "section": "Data Tables",
    "text": "Data Tables\nYou can display tables per the usual way in your blog:\n\n# display table with pandas\ndf[['Title', 'Worldwide_Gross', \n    'Production_Budget', 'Distributor', 'MPAA_Rating', 'IMDB_Rating', 'Rotten_Tomatoes_Rating']].head()\n\n\n\n\n\n\n\n\nTitle\nWorldwide_Gross\nProduction_Budget\nDistributor\nMPAA_Rating\nIMDB_Rating\nRotten_Tomatoes_Rating\n\n\n\n\n0\nThe Land Girls\n146083.0\n8000000.0\nGramercy\nR\n6.1\nNaN\n\n\n1\nFirst Love, Last Rites\n10876.0\n300000.0\nStrand\nR\n6.9\nNaN\n\n\n2\nI Married a Strange Person\n203134.0\n250000.0\nLionsgate\nNone\n6.8\nNaN\n\n\n3\nLet's Talk About Sex\n373615.0\n300000.0\nFine Line\nNone\nNaN\n13.0\n\n\n4\nSlam\n1087521.0\n1000000.0\nTrimark\nR\n3.4\n62.0"
  },
  {
    "objectID": "2020-02-20-test.html#images",
    "href": "2020-02-20-test.html#images",
    "title": "Fastpages Notebook Blog Post",
    "section": "Images",
    "text": "Images\n\nLocal Images\nYou can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax:\n![](my_icons/fastai_logo.png)\n\n\n\nRemote Images\nRemote images can be included with the following markdown syntax:\n![](https://image.flaticon.com/icons/svg/36/36686.svg)\n\n\n\nAnimated Gifs\nAnimated Gifs work, too!\n![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif)\n\n\n\nCaptions\nYou can include captions with markdown images like this:\n![](https://www.fast.ai/images/fastai_paper/show_batch.png \"Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/\")"
  },
  {
    "objectID": "2020-02-20-test.html#github-flavored-emojis",
    "href": "2020-02-20-test.html#github-flavored-emojis",
    "title": "Fastpages Notebook Blog Post",
    "section": "GitHub Flavored Emojis",
    "text": "GitHub Flavored Emojis\nTyping I give this post two :+1:! will render this:\nI give this post two :+1:!"
  },
  {
    "objectID": "2020-02-20-test.html#tweetcards",
    "href": "2020-02-20-test.html#tweetcards",
    "title": "Fastpages Notebook Blog Post",
    "section": "Tweetcards",
    "text": "Tweetcards\nTyping &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this:\n\ntwitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20"
  },
  {
    "objectID": "2020-02-20-test.html#youtube-videos",
    "href": "2020-02-20-test.html#youtube-videos",
    "title": "Fastpages Notebook Blog Post",
    "section": "Youtube Videos",
    "text": "Youtube Videos\nTyping &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this:\n\nyoutube: https://youtu.be/XfoYk_Z5AkI"
  },
  {
    "objectID": "2020-02-20-test.html#boxes-callouts",
    "href": "2020-02-20-test.html#boxes-callouts",
    "title": "Fastpages Notebook Blog Post",
    "section": "Boxes / Callouts",
    "text": "Boxes / Callouts\nTyping &gt; Warning: There will be no second warning! will render this:\n\nWarning: There will be no second warning!\n\nTyping &gt; Important: Pay attention! It's important. will render this:\n\nImportant: Pay attention! It’s important.\n\nTyping &gt; Tip: This is my tip. will render this:\n\nTip: This is my tip.\n\nTyping &gt; Note: Take note of this. will render this:\n\nNote: Take note of this.\n\nTyping &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs:\n\nNote: A doc link to an example website: fast.ai should also work fine."
  },
  {
    "objectID": "2020-02-20-test.html#footnotes",
    "href": "2020-02-20-test.html#footnotes",
    "title": "Fastpages Notebook Blog Post",
    "section": "Footnotes",
    "text": "Footnotes\nYou can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this:\n{% raw %}For example, here is a footnote {% fn 1 %}.\nAnd another {% fn 2 %}\n{{ 'This is the footnote.' | fndetail: 1 }}\n{{ 'This is the other footnote. You can even have a [link](www.github.com)!' | fndetail: 2 }}{% endraw %}\nFor example, here is a footnote {% fn 1 %}.\nAnd another {% fn 2 %}\n{{ ‘This is the footnote.’ | fndetail: 1 }} {{ ‘This is the other footnote. You can even have a link!’ | fndetail: 2 }}"
  }
]